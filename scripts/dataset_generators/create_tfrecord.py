import collections
import functools
import json
import os
import pickle
import glob
import re
import math
import sys

import numpy as np
import tensorflow.compat.v1 as tf


tf.enable_eager_execution()
tf.executing_eagerly()


import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--data", help="data dir", required=True, type=str)
parser.add_argument("--out", help="out dir", required=True, type=str)
parser.add_argument("--flex", help="Is this flex data?", action='store_true')
parser.add_argument("--mpm", help="Is this mpm data?", action='store_true')
parser.add_argument("--restpos", help="include respos?", action='store_true')
parser.add_argument("--reduced", help="reduced number of particles", required=False, default=-1, type=int)

parser.add_argument("--num_data", help="number of data to include", required=False, default=100, type=int)
parser.add_argument("--offset", help="data load offset", required=False, default=0, type=int)
parser.add_argument("--name", help="name of the output file", required=False, default="train", type=str)
parser.add_argument("--has_context", help="has context?", required=False, default=True, type=bool)


args = parser.parse_args()

if ( (args.mpm and args.flex) or (not args.mpm and not args.flex) ):
    sys.error("Please specify --mpm or --flex")



def rotate(p, quat):
    R = np.zeros((3, 3))
    a, b, c, d = quat[3], quat[0], quat[1], quat[2]
    R[0, 0] = a**2 + b**2 - c**2 - d**2
    R[0, 1] = 2 * b * c - 2 * a * d
    R[0, 2] = 2 * b * d + 2 * a * c
    R[1, 0] = 2 * b * c + 2 * a * d
    R[1, 1] = a**2 - b**2 + c**2 - d**2
    R[1, 2] = 2 * c * d - 2 * a * b
    R[2, 0] = 2 * b * d - 2 * a * c
    R[2, 1] = 2 * c * d + 2 * a * b
    R[2, 2] = a**2 - b**2 - c**2 + d**2

    return np.dot(R, p)


# copy of vispy_utils.py particlify_box()
def particlify_box(center, half_edge, quat):
    
    pos = []

    # initial spacing
    offset_height = 0.02
    offset_width1 = 0.02
    offset_width2 = 0.02
    
    half_width1 = half_edge[0]
    half_height = half_edge[1]
    half_width2 = half_edge[2]

    particle_count_height = math.ceil(half_height * 2 / offset_height)
    particle_count_width1 = math.ceil(half_width1 * 2 / offset_width1)
    particle_count_width2 = math.ceil(half_width2 * 2 / offset_width2)

    offset_height = half_height * 2 / particle_count_height
    offset_width1 = half_width1 * 2 / particle_count_width1
    offset_width2 = half_width2 * 2 / particle_count_width2


    local_bottom_corner_pos = np.array([-half_width1, -half_height, - half_width2])


    for h in range(0, particle_count_height + 1):
        for w in range(0, particle_count_width1):
            pos.append(local_bottom_corner_pos + np.array([offset_width1 * w, offset_height * h, 0]))
        for w in range(0, particle_count_width2):
            pos.append(local_bottom_corner_pos + np.array([half_width1 * 2, offset_height * h, offset_width2 * w]))
        for w in range(0, particle_count_width1):
            pos.append(local_bottom_corner_pos + np.array([half_width1 * 2 - offset_width2 * w, offset_height * h, half_width2 * 2]))
        for w in range(0, particle_count_width2):
            pos.append(local_bottom_corner_pos + np.array([0, offset_height * h, half_width2 * 2 - offset_width2 * w]))

    for r in range(1, particle_count_width1):
        for c in range(1, particle_count_width2):
            pos.append(local_bottom_corner_pos + np.array([offset_width1 * r, half_height * 2, offset_width2 * c]))
            pos.append(local_bottom_corner_pos + np.array([offset_width1 * r, 0, offset_width2 * c]))
        

    pos = np.asarray(pos, dtype=np.float64)
    
    for i in range(len(pos)):
        pos[i] = rotate(pos[i], quat)

    pos[:,0] += center[0]
    pos[:,1] += center[1]
    pos[:,2] += center[2]
        
    # pos = np.concatenate((pos, np.ones([len(pos), 1])), 1)
    
    return pos


"""
respos: if restpos is True, grip particles will be expanded to 6 dim
almost same as vispy_utils.py, respos is different
"""
def add_grips(positions, shape_states, half_edge, restpos=False):
    pos_all = []
    for r in range(len(positions)):
        pos_grip_iter = []
        for i in range(len(positions[0])):

            pos_grips = []

            for i_grip in range(len(shape_states[r, i])):

                pos = shape_states[r, i][i_grip][0:3]
                quat = shape_states[r, i][i_grip][6:10]
                pos_grip = particlify_box(pos, half_edge[i_grip], quat)

                if restpos: pos_grips.append(np.concatenate([pos_grip, pos_grip], axis=1))
                else : pos_grips.append(pos_grip)

            pos_grips = np.vstack(pos_grips)
            pos_grips = pos_grips.reshape(-1, pos_grips.shape[-1])


            pos_grip_iter.append(np.concatenate((positions[r,i], pos_grips), 0))

        pos_all.append(pos_grip_iter)

    pos_all = np.asarray(pos_all, dtype=np.float64)

    return pos_all


def generate_tfrecord_plb(data_name, writer_name, idx_start, idx_end, _HAS_CONTEXT=True, is_mpm=True, restpos=False):

    writer = tf.python_io.TFRecordWriter(writer_name)

    files = glob.glob(os.path.join(data_name, "*.npy"))
    files.sort(key = lambda f: int(re.sub('\D', '', f)))
    files = files[idx_start:idx_end]
    
    i = 0
    for file in files:
        print(f'{file}', end="\r",)
        d = np.load(file, allow_pickle=True).item()

        # # Ad-hoc method to concate 5 grip iterations in a sequence
        # positions_collapsed = d['positions'].reshape(-1, d['positions'].shape[2], d['positions'].shape[3])
        # d['positions'] = np.zeros((1, d['positions'].shape[0] * d['positions'].shape[1], d['positions'].shape[2], d['positions'].shape[3]))
        # d['positions'][0] = positions_collapsed

        # shape_states_collapsed = d['shape_states'].reshape(-1, d['shape_states'].shape[2], d['shape_states'].shape[3])
        # d['shape_states'] = np.zeros((1, d['shape_states'].shape[0] * d['shape_states'].shape[1], d['shape_states'].shape[2], d['shape_states'].shape[3]))
        # d['shape_states'][0] = shape_states_collapsed


        # Random sample to match MPM with FLEX
        if args.reduced > 0:
            assert args.reduced <= d['positions'].shape[2]
            random_indices = np.random.randint(d['positions'].shape[2], size=args.reduced)
            d['positions'] = d['positions'][:,:,random_indices]

        # If MPM and --restpos, need to extend position to 6 dim
        # If FLEX and not --restpos, remove restpos
        if is_mpm:
            if restpos:
                d_pos = np.concatenate([d['positions'][:,:,:,:3], d['positions'][:,:,:,:3]], axis=3)
            else:
                d_pos = d['positions'][:,:,:,:3]
        else:
            if not restpos:
                d_pos = d['positions'][:,:,:,3:]
            else:
                d_pos = d['positions']


        d['new_positions'] = add_grips(d_pos , d['shape_states'], d['scene_info'], restpos)

        
        n_particle_plasticine = len(d['positions'][0,0])
        
        positions = d['new_positions'][0].astype(np.float32)


        step_contexts = []
        for _ in range(0, len(positions)):
            if is_mpm:
                step_contexts.append(d['YS'])
                step_contexts.append(d['E'])
                step_contexts.append(d['nu'])
            else:
                step_contexts.append(d['clusterStiffness'])
                step_contexts.append(d['clusterPlasticThreshold'])
                step_contexts.append(d['clusterPlasticCreep'])


        positions = np.asarray(positions)
        step_contexts = np.asarray(step_contexts)

        # Create feature list
        positions_bytes_list = []
        for pos in positions: # per frame
            positions_bytes = pos.tobytes()
            positions_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[positions_bytes]))
            positions_bytes_list.append(positions_bytes)

        step_context_bytes_list = []
        for step_context in step_contexts: # per frame
            step_context_bytes = np.float32(step_context).tobytes()
            step_context_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[step_context_bytes]))
            step_context_bytes_list.append(step_context_bytes)

        positions_feature_list = tf.train.FeatureList(feature=positions_bytes_list)
        if _HAS_CONTEXT:
            step_context_feature_list = tf.train.FeatureList(feature=step_context_bytes_list)

        particle_type = np.ones([positions[0].shape[0]], dtype=np.int64)
        particle_type[n_particle_plasticine:] = 0
        particle_type = particle_type.tobytes()
        particle_type_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[particle_type]))

        key = np.int64(i)
        key_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[key]))

        sequence_dict = {'position': positions_feature_list, 'step_context': step_context_feature_list}

        context_dict = {'key': key_feature, 'particle_type': particle_type_feature}

        sequence_context = tf.train.Features(feature=context_dict)
        # now create a list of feature lists contained within dictionary
        sequence_list = tf.train.FeatureLists(feature_list=sequence_dict)

        example = tf.train.SequenceExample(context=sequence_context, feature_lists=sequence_list)

        writer.write(example.SerializeToString())

        i += 1

    writer.close()


def main():

    os.system('mkdir -p ' + args.out)

    generate_tfrecord_plb(
        args.data, 
        os.path.join(args.out, args.name + ".tfrecord"), 
        args.offset, 
        args.offset + args.num_data, 
        args.has_context,
        args.mpm,
        args.restpos
    )



if __name__ == "__main__":
    main()