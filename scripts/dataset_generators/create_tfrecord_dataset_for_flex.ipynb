{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f09443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7727512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "def rotate(p, quat):\n",
    "    R = np.zeros((3, 3))\n",
    "    a, b, c, d = quat[3], quat[0], quat[1], quat[2]\n",
    "    R[0, 0] = a**2 + b**2 - c**2 - d**2\n",
    "    R[0, 1] = 2 * b * c - 2 * a * d\n",
    "    R[0, 2] = 2 * b * d + 2 * a * c\n",
    "    R[1, 0] = 2 * b * c + 2 * a * d\n",
    "    R[1, 1] = a**2 - b**2 + c**2 - d**2\n",
    "    R[1, 2] = 2 * c * d - 2 * a * b\n",
    "    R[2, 0] = 2 * b * d - 2 * a * c\n",
    "    R[2, 1] = 2 * c * d + 2 * a * b\n",
    "    R[2, 2] = a**2 - b**2 - c**2 + d**2\n",
    "\n",
    "    return np.dot(R, p)\n",
    "\n",
    "\n",
    "def particlify_box(center, half_edge, quat):\n",
    "    \n",
    "    pos = []\n",
    "\n",
    "    # initial spacing\n",
    "    offset_height = 0.02\n",
    "    offset_width = 0.02\n",
    "    \n",
    "    half_width = half_edge[0] # assume width = depth\n",
    "    half_height = half_edge[1]\n",
    "\n",
    "    particle_count_height = math.ceil(half_height * 2 / offset_height)\n",
    "    particle_count_width = math.ceil(half_width * 2 / offset_width)\n",
    "\n",
    "    offset_height = half_height * 2 / particle_count_height\n",
    "    offset_width = half_width * 2 / particle_count_width\n",
    "\n",
    "\n",
    "    local_bottom_corner_pos = np.array([-half_width, -half_height, - half_width])\n",
    "\n",
    "\n",
    "    for h in range(0, particle_count_height + 1):\n",
    "        for w in range(0, particle_count_width):\n",
    "            pos.append(local_bottom_corner_pos + np.array([offset_width * w, offset_height * h, 0]))\n",
    "        for w in range(0, particle_count_width):\n",
    "            pos.append(local_bottom_corner_pos + np.array([half_width * 2, offset_height * h, offset_width * w]))\n",
    "        for w in range(0, particle_count_width):\n",
    "            pos.append(local_bottom_corner_pos + np.array([half_width * 2 - offset_width * w, offset_height * h, half_width * 2]))\n",
    "        for w in range(0, particle_count_width):\n",
    "            pos.append(local_bottom_corner_pos + np.array([0, offset_height * h, half_width * 2 - offset_width * w]))\n",
    "\n",
    "    for r in range(1, particle_count_width):\n",
    "        for c in range(1, particle_count_width):\n",
    "            pos.append(local_bottom_corner_pos + np.array([offset_width * r, half_height * 2, offset_width * c]))\n",
    "            pos.append(local_bottom_corner_pos + np.array([offset_width * r, 0, offset_width * c]))\n",
    "        \n",
    "\n",
    "    pos = np.asarray(pos, dtype=np.float64)\n",
    "    \n",
    "    for i in range(len(pos)):\n",
    "        pos[i] = rotate(pos[i], quat)\n",
    "\n",
    "    pos[:,0] += center[0]\n",
    "    pos[:,1] += center[1]\n",
    "    pos[:,2] += center[2]\n",
    "        \n",
    "    # pos = np.concatenate((pos, np.ones([len(pos), 1])), 1)\n",
    "    \n",
    "    return pos\n",
    "\n",
    "def add_grips(positions, shape_states, half_edge):\n",
    "    pos_all = []\n",
    "    for r in range(len(positions)):\n",
    "        pos_grip_iter = []\n",
    "        for i in range(len(positions[0])):\n",
    "\n",
    "            pos_grips = []\n",
    "\n",
    "            for i_grip in range(len(shape_states[r, i])):\n",
    "\n",
    "                pos = shape_states[r, i][i_grip][0:3]\n",
    "                quat = shape_states[r, i][i_grip][6:10]\n",
    "                pos_grip = particlify_box(pos, half_edge, quat)\n",
    "\n",
    "                pos_grips.append(pos_grip)\n",
    "\n",
    "            pos_grips = np.array(pos_grips)\n",
    "            pos_grips = pos_grips.reshape(-1, pos_grips.shape[-1])\n",
    "\n",
    "\n",
    "            pos_grip_iter.append(np.concatenate((positions[r,i], pos_grips), 0))\n",
    "        pos_all.append(pos_grip_iter)\n",
    "\n",
    "    pos_all = np.asarray(pos_all, dtype=np.float64)\n",
    "\n",
    "    return pos_all\n",
    "\n",
    "def add_grips(positions, shape_states, half_edge, has_restpos=False):\n",
    "    pos_all = []\n",
    "    for r in range(len(positions)):\n",
    "        pos_grip_iter = []\n",
    "        for i in range(len(positions[0])):\n",
    "\n",
    "            pos1 = shape_states[r, i][0][0:3]\n",
    "            quat1 = shape_states[r, i][0][6:10]\n",
    "            pos_grip1 = particlify_box(pos1, half_edge, quat1)\n",
    "\n",
    "            pos2 = shape_states[r, i][1][0:3]\n",
    "            quat2 = shape_states[r, i][1][6:10]\n",
    "            pos_grip2 = particlify_box(pos2, half_edge, quat2)\n",
    "            \n",
    "            if has_restpos: pos_grip_iter.append(np.concatenate((positions[r,i], np.concatenate([pos_grip1, pos_grip1], axis=1), np.concatenate([pos_grip2, pos_grip2], axis=1)), 0))\n",
    "            else:pos_grip_iter.append(np.concatenate((positions[r,i], pos_grip1, pos_grip2), 0))\n",
    "            \n",
    "        pos_all.append(pos_grip_iter)\n",
    "\n",
    "    pos_all = np.asarray(pos_all, dtype=np.float64)\n",
    "\n",
    "    return pos_all   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a365715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/issei/Documents/UCSD/SuLab/Neural_Simulation/scripts/data_generators/RiceGrip/RiceGrip/4009.npy\r"
     ]
    }
   ],
   "source": [
    "DATA_NAME = \"../../tmp/Finetube/CompressTube/FLEX/raw\"\n",
    "WRITER_DIR = \"../../tmp/Finetube/CompressTube/FLEX/data\"\n",
    "\n",
    "os.makedirs(WRITER_DIR, exist_ok=True)\n",
    "# WRITER_NAME = os.path.join(WRITER_DIR, \"train.tfrecord\")\n",
    "has_context = True\n",
    "has_restpos = True\n",
    "\n",
    "# generate_tfrecord(DATA_NAME, os.path.join(WRITER_DIR, \"test.tfrecord\"), 0, 1, has_context, has_restpos)\n",
    "\n",
    "generate_tfrecord(DATA_NAME, os.path.join(WRITER_DIR, \"train.tfrecord\"), 0, 4000, has_context, has_restpos)\n",
    "generate_tfrecord(DATA_NAME, os.path.join(WRITER_DIR, \"test.tfrecord\"), 4000, 4500, has_context, has_restpos)\n",
    "generate_tfrecord(DATA_NAME, os.path.join(WRITER_DIR, \"rollouts/train.tfrecord\"), 0, 10, has_context, has_restpos)\n",
    "generate_tfrecord(DATA_NAME, os.path.join(WRITER_DIR, \"rollouts/test.tfrecord\"), 4000, 4010, has_context, has_restpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9364e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecord(data_name, writer_name, idx_start, idx_end, _HAS_CONTEXT=True, has_restpos=False):\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(writer_name)\n",
    "\n",
    "    files = glob.glob(os.path.join(data_name, \"*.npy\"))\n",
    "    files.sort(key = lambda f: int(re.sub('\\D', '', f)))\n",
    "    files = files[idx_start:idx_end]\n",
    "    \n",
    "    i = 0\n",
    "    for file in files:\n",
    "        print(f'{file}', end=\"\\r\",)\n",
    "        d = np.load(file, allow_pickle=True).item()\n",
    "        \n",
    "        half_edge = np.array([0.15, 0.8, 0.15]) # FLEX\n",
    "        \n",
    "        \n",
    "        d['new_positions'] = add_grips(d['positions'] , d['shape_states'], d['scene_info'], has_restpos)\n",
    "        \n",
    "        n_particle_plasticine = len(d['positions'][0,0])\n",
    "        \n",
    "        positions = d['new_positions'][0].astype(np.float32)\n",
    "        \n",
    "        # FLEX rigid box is a unit cube centered at the origin\n",
    "        # First divide it by 5 to rescale it to 0.2 each dim\n",
    "        # Then translate it to place it in the center of [0,1] scene\n",
    "        positions /= 5\n",
    "        positions[:,:,0::3] += 0.5\n",
    "        positions[:,:,2::3] += 0.5\n",
    "        \n",
    "        # print(positions[0].min(axis=0), positions[0].max(axis=0))\n",
    "        step_contexts = []\n",
    "        for _ in range(0, len(positions)):\n",
    "            step_contexts.append(d['clusterStiffness'])\n",
    "            step_contexts.append(d['clusterPlasticThreshold'])\n",
    "            step_contexts.append(d['clusterPlasticCreep'])\n",
    "\n",
    "        positions = np.asarray(positions)\n",
    "        step_contexts = np.asarray(step_contexts)\n",
    "\n",
    "        # Create feature list\n",
    "        positions_bytes_list = []\n",
    "        for pos in positions: # per frame\n",
    "            positions_bytes = pos.tobytes()\n",
    "            positions_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[positions_bytes]))\n",
    "            positions_bytes_list.append(positions_bytes)\n",
    "\n",
    "        step_context_bytes_list = []\n",
    "        for step_context in step_contexts: # per frame\n",
    "            step_context_bytes = np.float32(step_context).tobytes()\n",
    "            step_context_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[step_context_bytes]))\n",
    "            step_context_bytes_list.append(step_context_bytes)\n",
    "\n",
    "        positions_feature_list = tf.train.FeatureList(feature=positions_bytes_list)\n",
    "        if _HAS_CONTEXT:\n",
    "            step_context_feature_list = tf.train.FeatureList(feature=step_context_bytes_list)\n",
    "\n",
    "        particle_type = np.ones([positions[0].shape[0]], dtype=np.int64)\n",
    "        particle_type[n_particle_plasticine:] += 2\n",
    "        particle_type = particle_type.tobytes()\n",
    "        particle_type_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[particle_type]))\n",
    "\n",
    "        key = np.int64(i)\n",
    "        key_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[key]))\n",
    "\n",
    "        sequence_dict = {'position': positions_feature_list, 'step_context': step_context_feature_list}\n",
    "\n",
    "        context_dict = {'key': key_feature, 'particle_type': particle_type_feature}\n",
    "\n",
    "        sequence_context = tf.train.Features(feature=context_dict)\n",
    "        # now create a list of feature lists contained within dictionary\n",
    "        sequence_list = tf.train.FeatureLists(feature_list=sequence_dict)\n",
    "\n",
    "        example = tf.train.SequenceExample(context=sequence_context, feature_lists=sequence_list)\n",
    "\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354e1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5e639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/issei/Documents/UCSD/SuLab/Neural_Simulation/tmp/FLEX_RiceGrip/mpm_data/0004.npy\r"
     ]
    }
   ],
   "source": [
    "DATA_NAME = \"/home/issei/Documents/UCSD/SuLab/Neural_Simulation/tmp/FLEX_RiceGrip/mpm_data\"\n",
    "WRITER_DIR = \"/home/issei/Documents/UCSD/SuLab/Neural_Simulation/tmp/FLEX_RiceGrip/mpm_data\"\n",
    "\n",
    "os.makedirs(WRITER_DIR, exist_ok=True)\n",
    "# WRITER_NAME = os.path.join(WRITER_DIR, \"train.tfrecord\")\n",
    "has_context = True\n",
    "has_restpos = True\n",
    "\n",
    "generate_tfrecord_plb(DATA_NAME, os.path.join(WRITER_DIR, \"4.tfrecord\"), 0, 1, has_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a18a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecord_plb(data_name, writer_name, idx_start, idx_end, _HAS_CONTEXT=True):\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(writer_name)\n",
    "\n",
    "    files = glob.glob(os.path.join(data_name, \"*.npy\"))\n",
    "    files.sort(key = lambda f: int(re.sub('\\D', '', f)))\n",
    "    files = files[idx_start:idx_end]\n",
    "    \n",
    "    i = 0\n",
    "    for file in files:\n",
    "        print(f'{file}', end=\"\\r\",)\n",
    "        d = np.load(file, allow_pickle=True).item()\n",
    "        \n",
    "        half_edge = np.array([0.03, 0.16, 0.03]) # PLB\n",
    "        \n",
    "        # print(np.concatenate([d['positions'][:,:,:,:3], d['positions'][:,:,:,:3]], axis=3).shape)\n",
    "        d['new_positions'] = add_grips(np.concatenate([d['positions'][:,:,:,:3], d['positions'][:,:,:,:3]], axis=3) , d['shape_states'], half_edge, has_restpos)\n",
    "        \n",
    "        n_particle_plasticine = len(d['positions'][0,0])\n",
    "        \n",
    "        positions = d['new_positions'][0].astype(np.float32)\n",
    "        \n",
    "        # print(positions[0].min(axis=0), positions[0].max(axis=0))\n",
    "        step_contexts = []\n",
    "        for _ in range(0, len(positions)):\n",
    "            step_contexts.append(d['YS'])\n",
    "            step_contexts.append(d['E'])\n",
    "            step_contexts.append(d['nu'])\n",
    "\n",
    "        positions = np.asarray(positions)\n",
    "        step_contexts = np.asarray(step_contexts)\n",
    "\n",
    "        # Create feature list\n",
    "        positions_bytes_list = []\n",
    "        for pos in positions: # per frame\n",
    "            positions_bytes = pos.tobytes()\n",
    "            positions_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[positions_bytes]))\n",
    "            positions_bytes_list.append(positions_bytes)\n",
    "\n",
    "        step_context_bytes_list = []\n",
    "        for step_context in step_contexts: # per frame\n",
    "            step_context_bytes = np.float32(step_context).tobytes()\n",
    "            step_context_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[step_context_bytes]))\n",
    "            step_context_bytes_list.append(step_context_bytes)\n",
    "\n",
    "        positions_feature_list = tf.train.FeatureList(feature=positions_bytes_list)\n",
    "        if _HAS_CONTEXT:\n",
    "            step_context_feature_list = tf.train.FeatureList(feature=step_context_bytes_list)\n",
    "\n",
    "        particle_type = np.ones([positions[0].shape[0]], dtype=np.int64)\n",
    "        particle_type[n_particle_plasticine:] += 2\n",
    "        particle_type = particle_type.tobytes()\n",
    "        particle_type_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[particle_type]))\n",
    "\n",
    "        key = np.int64(i)\n",
    "        key_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[key]))\n",
    "\n",
    "        sequence_dict = {'position': positions_feature_list, 'step_context': step_context_feature_list}\n",
    "\n",
    "        context_dict = {'key': key_feature, 'particle_type': particle_type_feature}\n",
    "\n",
    "        sequence_context = tf.train.Features(feature=context_dict)\n",
    "        # now create a list of feature lists contained within dictionary\n",
    "        sequence_list = tf.train.FeatureLists(feature_list=sequence_dict)\n",
    "\n",
    "        example = tf.train.SequenceExample(context=sequence_context, feature_lists=sequence_list)\n",
    "\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7d9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
