{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f09443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7727512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "def rotate(p, quat):\n",
    "    R = np.zeros((3, 3))\n",
    "    a, b, c, d = quat[3], quat[0], quat[1], quat[2]\n",
    "    R[0, 0] = a**2 + b**2 - c**2 - d**2\n",
    "    R[0, 1] = 2 * b * c - 2 * a * d\n",
    "    R[0, 2] = 2 * b * d + 2 * a * c\n",
    "    R[1, 0] = 2 * b * c + 2 * a * d\n",
    "    R[1, 1] = a**2 - b**2 + c**2 - d**2\n",
    "    R[1, 2] = 2 * c * d - 2 * a * b\n",
    "    R[2, 0] = 2 * b * d - 2 * a * c\n",
    "    R[2, 1] = 2 * c * d + 2 * a * b\n",
    "    R[2, 2] = a**2 - b**2 - c**2 + d**2\n",
    "\n",
    "    return np.dot(R, p)\n",
    "\n",
    "\n",
    "# copy of vispy_utils.py particlify_box()\n",
    "def particlify_box(center, half_edge, quat):\n",
    "    \n",
    "    pos = []\n",
    "\n",
    "    # initial spacing\n",
    "    offset_height = 0.02\n",
    "    offset_width1 = 0.02\n",
    "    offset_width2 = 0.02\n",
    "    \n",
    "    half_width1 = half_edge[0]\n",
    "    half_height = half_edge[1]\n",
    "    half_width2 = half_edge[2]\n",
    "\n",
    "    particle_count_height = math.ceil(half_height * 2 / offset_height)\n",
    "    particle_count_width1 = math.ceil(half_width1 * 2 / offset_width1)\n",
    "    particle_count_width2 = math.ceil(half_width2 * 2 / offset_width2)\n",
    "\n",
    "    offset_height = half_height * 2 / particle_count_height\n",
    "    offset_width1 = half_width1 * 2 / particle_count_width1\n",
    "    offset_width2 = half_width2 * 2 / particle_count_width2\n",
    "\n",
    "\n",
    "    local_bottom_corner_pos = np.array([-half_width1, -half_height, - half_width2])\n",
    "\n",
    "\n",
    "    for h in range(0, particle_count_height + 1):\n",
    "        for w in range(0, particle_count_width1):\n",
    "            pos.append(local_bottom_corner_pos + np.array([offset_width1 * w, offset_height * h, 0]))\n",
    "        for w in range(0, particle_count_width2):\n",
    "            pos.append(local_bottom_corner_pos + np.array([half_width1 * 2, offset_height * h, offset_width2 * w]))\n",
    "        for w in range(0, particle_count_width1):\n",
    "            pos.append(local_bottom_corner_pos + np.array([half_width1 * 2 - offset_width2 * w, offset_height * h, half_width2 * 2]))\n",
    "        for w in range(0, particle_count_width2):\n",
    "            pos.append(local_bottom_corner_pos + np.array([0, offset_height * h, half_width2 * 2 - offset_width2 * w]))\n",
    "\n",
    "    for r in range(1, particle_count_width1):\n",
    "        for c in range(1, particle_count_width2):\n",
    "            pos.append(local_bottom_corner_pos + np.array([offset_width1 * r, half_height * 2, offset_width2 * c]))\n",
    "            pos.append(local_bottom_corner_pos + np.array([offset_width1 * r, 0, offset_width2 * c]))\n",
    "        \n",
    "\n",
    "    pos = np.asarray(pos, dtype=np.float64)\n",
    "    \n",
    "    for i in range(len(pos)):\n",
    "        pos[i] = rotate(pos[i], quat)\n",
    "\n",
    "    pos[:,0] += center[0]\n",
    "    pos[:,1] += center[1]\n",
    "    pos[:,2] += center[2]\n",
    "        \n",
    "    # pos = np.concatenate((pos, np.ones([len(pos), 1])), 1)\n",
    "    \n",
    "    return pos\n",
    "\n",
    "def add_grips(positions, shape_states, half_edge, restpos=False):\n",
    "    pos_all = []\n",
    "    for r in range(len(positions)):\n",
    "        pos_grip_iter = []\n",
    "        for i in range(len(positions[0])):\n",
    "\n",
    "            pos_grips = []\n",
    "\n",
    "            for i_grip in range(len(shape_states[r, i])):\n",
    "\n",
    "                pos = shape_states[r, i][i_grip][0:3]\n",
    "                quat = shape_states[r, i][i_grip][6:10]\n",
    "                pos_grip = particlify_box(pos, half_edge[i_grip], quat)\n",
    "\n",
    "                if restpos: pos_grips.append(np.concatenate([pos_grip, pos_grip], axis=1))\n",
    "                else : pos_grips.append(pos_grip)\n",
    "\n",
    "            pos_grips = np.vstack(pos_grips)\n",
    "            pos_grips = pos_grips.reshape(-1, pos_grips.shape[-1])\n",
    "\n",
    "\n",
    "            pos_grip_iter.append(np.concatenate((positions[r,i], pos_grips), 0))\n",
    "\n",
    "        pos_all.append(pos_grip_iter)\n",
    "\n",
    "    pos_all = np.asarray(pos_all, dtype=np.float64)\n",
    "\n",
    "    return pos_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5e639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "DATA_PATICLE_PATH = \"/home/issei/Documents/UCSD/SuLab/Neural_Simulation/tmp/Finetune/RiceGripRandom/FLEX/raw\"\n",
    "DATA_DEPTH_PATH = \"/home/issei/Documents/UCSD/SuLab/Neural_Simulation/tmp/Finetune/RiceGripRandom/MPM\"\n",
    "OUT_PATH = \"/home/issei/Documents/UCSD/SuLab/Neural_Simulation/tmp/Finetune/RiceGripRandom/FLEX/finetune/data\"\n",
    "\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "# WRITER_NAME = os.path.join(WRITER_DIR, \"train.tfrecord\")\n",
    "has_context = True\n",
    "has_restpos = True\n",
    "is_mpm = False\n",
    "\n",
    "generate_tfrecord(DATA_PATICLE_PATH, DATA_DEPTH_PATH, os.path.join(OUT_PATH, \"train.tfrecord\"), 0, 8, has_context, has_restpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a18a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecord(data_particle_path, data_depth_path, writer_name, idx_start, idx_end, _HAS_CONTEXT=True, _HAS_RESTPOS=True):\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(writer_name)\n",
    "    \n",
    "    files_particle = glob.glob(os.path.join(data_particle_path, \"*.npy\"))\n",
    "    \n",
    "    files_depthsT = glob.glob(os.path.join(data_depth_path, \"depth_top/*/*.npy\"))\n",
    "    files_depthsF = glob.glob(os.path.join(data_depth_path, \"depth_front/*/*.npy\"))\n",
    "    files_depthsR = glob.glob(os.path.join(data_depth_path, \"depth_right/*/*.npy\"))\n",
    "    files_depthsL = glob.glob(os.path.join(data_depth_path, \"depth_left/*/*.npy\"))\n",
    "    files_depthsB = glob.glob(os.path.join(data_depth_path, \"depth_back/*/*.npy\"))\n",
    "    \n",
    "    files_particle.sort(key = lambda f: int(re.sub('\\\\D', '', f)))\n",
    "    \n",
    "    files_depthsT.sort(key = lambda f: int(re.sub('\\\\D', '', f)))\n",
    "    files_depthsF.sort(key = lambda f: int(re.sub('\\\\D', '', f)))\n",
    "    files_depthsR.sort(key = lambda f: int(re.sub('\\\\D', '', f)))\n",
    "    files_depthsL.sort(key = lambda f: int(re.sub('\\\\D', '', f)))\n",
    "    files_depthsB.sort(key = lambda f: int(re.sub('\\\\D', '', f)))\n",
    "    \n",
    "    for file_i in range(idx_start, idx_end):\n",
    "#         file_particle = os.path.join(data_particle_path, \"{:0>4}.npy\".format(str(file_i)))\n",
    "#         file_depth = os.path.join(data_depth_path, \"{:0>4}.npy\".format(str(file_i)))\n",
    "\n",
    "        file_particle = files_particle[file_i + 4000]\n",
    "    \n",
    "        file_depthT = files_depthsT[file_i]\n",
    "        file_depthF = files_depthsF[file_i]\n",
    "        file_depthR = files_depthsR[file_i]\n",
    "        file_depthL = files_depthsL[file_i]\n",
    "        file_depthB = files_depthsB[file_i]\n",
    "        \n",
    "#         print(file_particle)\n",
    "#         print(file_depth)\n",
    "\n",
    "        \n",
    "        \n",
    "        d = np.load(file_particle, allow_pickle=True).item()\n",
    "        \n",
    "        d_depthT = np.load(file_depthT, allow_pickle=True)\n",
    "        d_depthF = np.load(file_depthF, allow_pickle=True)\n",
    "        d_depthR = np.load(file_depthR, allow_pickle=True)\n",
    "        d_depthL = np.load(file_depthL, allow_pickle=True)\n",
    "        d_depthB = np.load(file_depthB, allow_pickle=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(d_particle)\n",
    "#         print(d_depth.shape)\n",
    "#         print(d_depth.mean())\n",
    "        \n",
    "        # print(f'{file_i}', end=\"\\r\",)\n",
    "        print(f'{file_i}')\n",
    "        \n",
    "\n",
    "        \n",
    "        d['new_positions'] = add_grips(np.concatenate([d['positions'][:,:,:,:3], d['positions'][:,:,:,:3]], axis=3) , d['shape_states'], d['scene_info'], _HAS_RESTPOS)\n",
    "        \n",
    "        n_particle_plasticine = len(d['positions'][0,0])\n",
    "        \n",
    "        positions = d['new_positions'][0].astype(np.float32)\n",
    "        \n",
    "        # print(positions[0].min(axis=0), positions[0].max(axis=0))\n",
    "        step_contexts = []\n",
    "        for _ in range(0, len(positions)):\n",
    "            if is_mpm:\n",
    "                step_contexts.append(d['YS'])\n",
    "                step_contexts.append(d['E'])\n",
    "                step_contexts.append(d['nu'])\n",
    "            else:\n",
    "                step_contexts.append(d['clusterStiffness'])\n",
    "                step_contexts.append(d['clusterPlasticThreshold'])\n",
    "                step_contexts.append(d['clusterPlasticCreep'])\n",
    "\n",
    "        positions = np.asarray(positions)\n",
    "        step_contexts = np.asarray(step_contexts)\n",
    "\n",
    "        # Create feature list\n",
    "        positions_bytes_list = []\n",
    "        for pos in positions: # per frame\n",
    "            positions_bytes = pos.tobytes()\n",
    "            positions_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[positions_bytes]))\n",
    "            positions_bytes_list.append(positions_bytes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        depth_bytes_listF = []\n",
    "        for dep in d_depthF: # per frame\n",
    "            depth_bytes = dep.tobytes()\n",
    "            depth_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[depth_bytes]))\n",
    "            depth_bytes_listF.append(depth_bytes)\n",
    "        \n",
    "        depth_bytes_listT = []\n",
    "        for dep in d_depthT: # per frame\n",
    "            depth_bytes = dep.tobytes()\n",
    "            depth_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[depth_bytes]))\n",
    "            depth_bytes_listT.append(depth_bytes)\n",
    "        \n",
    "        depth_bytes_listR = []\n",
    "        for dep in d_depthR: # per frame\n",
    "            depth_bytes = dep.tobytes()\n",
    "            depth_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[depth_bytes]))\n",
    "            depth_bytes_listR.append(depth_bytes)\n",
    "        \n",
    "        depth_bytes_listL = []\n",
    "        for dep in d_depthL: # per frame\n",
    "            depth_bytes = dep.tobytes()\n",
    "            depth_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[depth_bytes]))\n",
    "            depth_bytes_listL.append(depth_bytes)\n",
    "        \n",
    "        depth_bytes_listB = []\n",
    "        for dep in d_depthB: # per frame\n",
    "            depth_bytes = dep.tobytes()\n",
    "            depth_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[depth_bytes]))\n",
    "            depth_bytes_listB.append(depth_bytes)\n",
    "            \n",
    "        \n",
    "\n",
    "        step_context_bytes_list = []\n",
    "        for step_context in step_contexts: # per frame\n",
    "            step_context_bytes = np.float32(step_context).tobytes()\n",
    "            step_context_bytes = tf.train.Feature(bytes_list = tf.train.BytesList(value=[step_context_bytes]))\n",
    "            step_context_bytes_list.append(step_context_bytes)\n",
    "\n",
    "        positions_feature_list = tf.train.FeatureList(feature=positions_bytes_list)\n",
    "        depth_bytes_listT = tf.train.FeatureList(feature=depth_bytes_listT)\n",
    "        depth_bytes_listF = tf.train.FeatureList(feature=depth_bytes_listF)\n",
    "        depth_bytes_listR = tf.train.FeatureList(feature=depth_bytes_listR)\n",
    "        depth_bytes_listL = tf.train.FeatureList(feature=depth_bytes_listL)\n",
    "        depth_bytes_listB = tf.train.FeatureList(feature=depth_bytes_listB)\n",
    "        \n",
    "        if _HAS_CONTEXT:\n",
    "            step_context_feature_list = tf.train.FeatureList(feature=step_context_bytes_list)\n",
    "\n",
    "        particle_type = np.ones([positions[0].shape[0]], dtype=np.int64)\n",
    "        particle_type[n_particle_plasticine:] += 2\n",
    "        particle_type = particle_type.tobytes()\n",
    "        particle_type_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[particle_type]))\n",
    "\n",
    "        key = np.int64(file_i)\n",
    "        key_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[key]))\n",
    "\n",
    "        # sequence_dict = {'position': positions_feature_list, 'step_context': step_context_feature_list, 'depths': depth_bytes_list}\n",
    "        sequence_dict = {'position': positions_feature_list, 'step_context': step_context_feature_list, 'depthsT': depth_bytes_listT, 'depthsF': depth_bytes_listF, 'depthsR': depth_bytes_listR, 'depthsL': depth_bytes_listL, 'depthsB': depth_bytes_listB}\n",
    "\n",
    "\n",
    "        context_dict = {'key': key_feature, 'particle_type': particle_type_feature}\n",
    "\n",
    "        sequence_context = tf.train.Features(feature=context_dict)\n",
    "        # now create a list of feature lists contained within dictionary\n",
    "        sequence_list = tf.train.FeatureLists(feature_list=sequence_dict)\n",
    "\n",
    "        example = tf.train.SequenceExample(context=sequence_context, feature_lists=sequence_list)\n",
    "\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "        file_i += 1\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7d9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
